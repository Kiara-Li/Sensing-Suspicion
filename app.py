import streamlit as st
import torch
import numpy as np
import pandas as pd
import plotly.express as px
from transformers import RobertaTokenizer, RobertaForSequenceClassification
import re

# é¡µé¢åŸºç¡€é…ç½®
st.set_page_config(
    page_title="Sensing Suspicion", 
    page_icon="ğŸ•µï¸â€â™€ï¸", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- æ ¸å¿ƒå·¥å…·å‡½æ•° ---

@st.cache_resource
def load_model():
    # --- ä¿®æ”¹å‰ (æœ¬åœ°è·¯å¾„) ---
    # model_path = "models/creepy_roberta"
    
    # --- ä¿®æ”¹å (Hugging Face ID) ---
    # è¯·æŠŠä¸‹é¢çš„ "ä½ çš„HFç”¨æˆ·å/æ¨¡å‹å" æ¢æˆä½ åˆšæ‰åˆ›å»ºçš„çœŸå® ID
    model_path = "KiaraLi2025/creepy-roberta" 
    
    st.write(f"Loading model from Hugging Face: {model_path} ...") # åŠ ä¸ªæç¤ºè®©æˆ‘ä»¬çŸ¥é“å®ƒåœ¨å·¥ä½œ
    
    try:
        tokenizer = RobertaTokenizer.from_pretrained(model_path)
        model = RobertaForSequenceClassification.from_pretrained(model_path)
        return tokenizer, model
    except Exception as e:
        st.error(f"Model loading failed!\nError message: {e}")
        return None, None

def get_prediction_score(text, tokenizer, model):
    """è¿”å› 'Creepy' (Label 1) çš„æ¦‚ç‡ (0.0 - 1.0)"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    return probs[0][1].item()

def split_into_segments(text):
    """ç®€å•çš„åˆ†å¥é€»è¾‘"""
    # æŒ‰ . ! ? ä»¥åŠæ¢è¡Œç¬¦åˆ‡åˆ†
    segments = re.split(r'(?<=[.!?\n])\s+', text)
    return [s.strip() for s in segments if len(s) > 5] # è¿‡æ»¤æ‰å¤ªçŸ­çš„

# --- ç•Œé¢é€»è¾‘ ---

# åŠ è½½æ¨¡å‹
tokenizer, model = load_model()

st.title("ğŸ•µï¸â€â™€ï¸ Sensing Suspicion")
st.markdown("### A Neural Network for Detecting 'Creepy Signals'")

with st.sidebar:
    st.write("### About This Project")
    st.info(
        "This project aims to use **RoBERTa** to detect subtle *creepy signals* in everyday narratives. "
        "By training the model on Reddit posts from subreddits such as **r/LetsNotMeet** and "
        "**r/TwoSentenceHorror**, it learns linguistic patterns associated with unsafe or suspicious "
        "situations, allowing us to predict whether new text may indicate a potentially dangerous encounter."
    )
    st.write("---")
    st.write("**Accuracy:** 95.2%")
    st.write("**Status:** Trained & Ready")

# ä¸»ç•Œé¢å¸ƒå±€
col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("ğŸ“ Input Narrative")
    user_input = st.text_area(
        "Paste a story here to analyze...", 
        height=400, 
        placeholder="It was late at night, and I was walking home alone..."
    )
    
    analyze_btn = st.button("ğŸ” Analyze for Suspicion", type="primary")

if analyze_btn and user_input and tokenizer and model:
    # 1. å…¨å±€åˆ†æ
    global_score = get_prediction_score(user_input, tokenizer, model)
    
    # 2. é€å¥åˆ†æ (ç”¨äºå¼ åŠ›å¼§)
    segments = split_into_segments(user_input)
    segment_scores = []
    
    # è¿›åº¦æ¡
    progress_bar = st.progress(0)
    for i, seg in enumerate(segments):
        score = get_prediction_score(seg, tokenizer, model)
        segment_scores.append(score)
        progress_bar.progress((i + 1) / len(segments))
    progress_bar.empty()

    # --- å³ä¾§ç»“æœå±•ç¤º ---
    with col2:
        st.subheader("ğŸ“Š Analysis Results")
        
        # ä»ªè¡¨ç›˜
        score_color = "red" if global_score > 0.7 else "orange" if global_score > 0.4 else "green"
        st.markdown(
            f"""
            <div style="text-align: center; margin-bottom: 20px;">
                <h1 style="color:{score_color}; font-size: 60px; margin:0;">{global_score:.1%}</h1>
                <p>Creepy Index (Overall)</p>
            </div>
            """, 
            unsafe_allow_html=True
        )

        if global_score > 0.7:
            st.error("âš ï¸ **High Suspicion Detected!** The model flagged significant unsafe patterns.")
        elif global_score > 0.4:
            st.warning("ğŸ¤” **Unsettling Tone.** Some parts of the story feel suspicious.")
        else:
            st.success("âœ… **Safe Narrative.** This reads like a normal everyday story.")

        st.divider()

        # å¼ åŠ›å¼§å¯è§†åŒ–
        st.markdown("#### ğŸ“ˆ Narrative Tension Arc")
        chart_data = pd.DataFrame({
            'Segment': range(1, len(segments) + 1),
            'Creepy Score': segment_scores,
            'Text': [s[:50] + "..." for s in segments] # ç¼©ç•¥æ–‡æœ¬
        })
        
        fig = px.line(chart_data, x='Segment', y='Creepy Score', 
                      markers=True, 
                      hover_data=['Text'],
                      line_shape='spline') # å¹³æ»‘æ›²çº¿
        
        # çº¢è‰²è­¦æˆ’åŒº
        fig.add_hrect(y0=0.8, y1=1.0, line_width=0, fillcolor="red", opacity=0.1)
        fig.update_yaxes(range=[0, 1.05])
        st.plotly_chart(fig, use_container_width=True)

    # --- åº•éƒ¨ï¼šé«˜äº®æ–‡æœ¬å±•ç¤º ---
    st.divider()
    st.subheader("ğŸ”¦ Contextual Highlighter")
    st.caption("Sentences flagged as 'Creepy' (>70%) are highlighted in red.")

    annotated_text = ""
    for seg, score in zip(segments, segment_scores):
        if score > 0.7:
            # çº¢è‰²é«˜äº®
            annotated_text += f'<span style="background-color: #ffcccc; padding: 2px 5px; border-radius: 5px; border: 1px solid #ff0000;">{seg}</span> '
        elif score > 0.4:
             # é»„è‰²é«˜äº®
            annotated_text += f'<span style="background-color: #fff4cc; padding: 2px 5px; border-radius: 5px;">{seg}</span> '
        else:
            annotated_text += f'{seg} '
            
    st.markdown(f'<div style="line-height: 1.6;">{annotated_text}</div>', unsafe_allow_html=True)

elif analyze_btn and not user_input:
    st.warning("Please paste some text first!")